{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data = pd.read_csv(sys.stdin, sep=',', header=None)\n",
    "df_data = pd.read_csv('dataset1.dat', sep=',', header=None)\n",
    "data_np = np.array(df_data)\n",
    "canopy_data = df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists for canopy function\n",
    "\n",
    "random_centroids_list = []\n",
    "total_distance = []\n",
    "sum_distance = []\n",
    "centroids = []\n",
    "\n",
    "\n",
    "def canopy(data, k):\n",
    "    if k == 1:\n",
    "        # Random centroid taken from dataset\n",
    "        random_point = (canopy_data.sample(1))\n",
    "        centroids.append(random_point)\n",
    "        # print(centroids)\n",
    "        return centroids\n",
    "\n",
    "    else:\n",
    "        # Random centroid taken from dataset\n",
    "        random_point = (canopy_data.sample(1))\n",
    "        centroids.append(random_point)\n",
    "        data = canopy_data.drop(random_point.index, axis=0, inplace=True)\n",
    "\n",
    "        for i in range(k - 1):\n",
    "            c_distance = distance.cdist(canopy_data, random_point, 'euclidean')\n",
    "            sum_distance.append(c_distance)\n",
    "            if k > 1:\n",
    "                total_distance = np.add(sum_distance, c_distance)\n",
    "                sum_distance.clear()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            index = np.argmax(c_distance)\n",
    "            # print(index)\n",
    "            new_centroid = canopy_data.iloc[[index]]\n",
    "            centroids.append(new_centroid)\n",
    "            data = canopy_data.drop(new_centroid.index, axis=0, inplace=True)\n",
    "\n",
    "        # convert to numpy array\n",
    "        centroid_array = np.asarray(centroids)\n",
    "        centroids.clear()\n",
    "        return centroid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows and Colunms of particular data\n",
    "n = df_data.shape[0]\n",
    "# Number of features in the data\n",
    "c = df_data.shape[1]\n",
    "\n",
    "# Cluster Assignment function of Kmeans\n",
    "def assign_cluster(k, dataset, centers_new):\n",
    "    dist = {}\n",
    "    # print(\"kkk\",k)\n",
    "    clusters = np.zeros(n)\n",
    "    distances = np.zeros((n, k))\n",
    "    # print(distances.shape)\n",
    "    for i in range(k):\n",
    "        dist[i] = []\n",
    "        distances[:, i] = np.linalg.norm(dataset - centers_new[i], axis=1)\n",
    "    # print(distances)\n",
    "    clusters = np.argmin(distances, axis=1)\n",
    "    # print(clusters)\n",
    "    # centers_old = deepcopy(centers_new)\n",
    "    # print(\"old\",centers_old)\n",
    "    # print(clusters)\n",
    "\n",
    "    # Assign cluster points to each centroids and store it as a dictonary\n",
    "    for c in range(len(clusters)):\n",
    "        # print(range(len(clusters)))\n",
    "        if clusters[c] not in dist:\n",
    "            dist[clusters[c]] = [dataset[c]]\n",
    "            # print(clusters[c])\n",
    "        else:\n",
    "            dist[clusters[c]].append(dataset[c])\n",
    "    # print(\"dist\",clusters)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After assign cluster function, new centroids gets updated\n",
    "def update_cluster(dist, centers_new):\n",
    "    for k1, v1 in dist.items():\n",
    "        # print(k1,v1)\n",
    "        centers_new[k1] = np.mean(v1, axis=0)\n",
    "    # print(\"update\",centers_new)\n",
    "    return centers_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouettes method calculates silhouettes score and returns mean si.\n",
    "def silhouettes(k_Means_output):\n",
    "    a_i = []\n",
    "    b_i = []\n",
    "    list_bi_mean = []\n",
    "    si = []\n",
    "    # sc = []\n",
    "    # a\n",
    "    for k, v in k_Means_output.items():\n",
    "\n",
    "        b = list(k_Means_output[k])\n",
    "        # print(b)\n",
    "        for p1 in b:\n",
    "            intra_distance = 0\n",
    "            for p2 in b:\n",
    "                if tuple(p1) != tuple(p2):\n",
    "                    intra_distance += distance.euclidean(tuple(p1), tuple(p2))\n",
    "                else:\n",
    "                    continue\n",
    "            if len(b) == 1:\n",
    "                a_i_mean = intra_distance\n",
    "            else:\n",
    "                a_i_mean = intra_distance / (len(k_Means_output[k]) - 1)\n",
    "            a_i.append(a_i_mean)\n",
    "\n",
    "            for key, val in k_Means_output.items():\n",
    "                if key != k:\n",
    "                    inter_cluster_distance = 0\n",
    "\n",
    "                    c = list(k_Means_output[key])\n",
    "                    for p3 in c:\n",
    "                        inter_cluster_distance += distance.euclidean(tuple(p1), tuple(p3))\n",
    "                else:\n",
    "                    continue\n",
    "                b_i_mean = inter_cluster_distance / len(k_Means_output[key])\n",
    "                list_bi_mean.append(b_i_mean)\n",
    "            b_i.append(min(list_bi_mean))\n",
    "            # print(list_bi_mean)\n",
    "\n",
    "            # for i in range(len(a_i)):\n",
    "            s_i = (min(list_bi_mean) - a_i_mean) / max(min(list_bi_mean), a_i_mean)\n",
    "            si.append(s_i)\n",
    "    mean_si = np.mean(si)\n",
    "    # sc.append(mean_si)\n",
    "    return mean_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means function calculates and recalculates centroids and until centroids #converge\n",
    "def k_means(dataset, previous_centroids, k):\n",
    "    # dist = {}\n",
    "    count = 0\n",
    "    # print(len()previous_centroids)\n",
    "    centers_old = np.zeros((len(previous_centroids), c))  # to store old centers\n",
    "    centers_new = deepcopy(previous_centroids)  # Store new centers\n",
    "\n",
    "    dataset.shape\n",
    "    # clusters = np.zeros(n)\n",
    "    # distances = np.zeros((n,k))\n",
    "    # print(centers_new.shape,centers_old.shape)\n",
    "    error = np.linalg.norm(centers_new - centers_old)\n",
    "    while error != 0:\n",
    "        # print('yes')\n",
    "        count += 1\n",
    "        # print(centers_new)\n",
    "        dist = assign_cluster(k, dataset, centers_new)\n",
    "        # Old centeroids are centroids of previous iteration\n",
    "        # new centroids are newly genrated centroids for next iteration\n",
    "        centers_old = deepcopy(centers_new)\n",
    "        centers_new = update_cluster(dist, centers_new)\n",
    "        # print(centers_new,centers_old)\n",
    "        error = np.linalg.norm(centers_new - centers_old)\n",
    "        # print(\"error\",error)\n",
    "    dist = assign_cluster(k, dataset, centers_new)\n",
    "    # centers_old = np.zeros(len(previous_centroids))\n",
    "\n",
    "    return dist, centers_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -771.2 -1582.8]\n",
      "[-1582.  -1582.7]\n"
     ]
    }
   ],
   "source": [
    "# To handle an anomalies in data such as nan, string values.\n",
    "# It exists if values are empty or non real \n",
    "if df_data.isnull().values.any() or ~df_data.applymap(np.real).all().all():\n",
    "    exit()\n",
    "else:\n",
    "    # Else entire k-means algorithm runs\n",
    "\n",
    "    k = [2,3,4,5,6]\n",
    "    sc = []\n",
    "    for i in k:\n",
    "        #  canopy_centroids = np.zeros((i,c))\n",
    "        #  canopy_centroids = canopy(canopy_data, i)\n",
    "        centroid_store = []\n",
    "        centroid_store = data_np[np.random.randint(0, n - 1, size=i)]\n",
    "        final_kmeans, new = k_means(data_np, centroid_store, i)\n",
    "        sil_score = silhouettes(final_kmeans)\n",
    "        # print(i,sil_score)\n",
    "        sc.append(sil_score)\n",
    "    # print(sc)\n",
    "    # for k1, v1 in k_Means_output.items():\n",
    "    #     print(k1, len(v1))\n",
    "\n",
    "    max_sc = max(sc)\n",
    "    # print(max_sc)\n",
    "    optimum_k = k[sc.index(max_sc)]\n",
    "    # print(optimum_k)\n",
    "    # final_kmeans, final_centroids = k_means(data_np, canopy_centroids, optimum_k)\n",
    "\n",
    "    # Anomaly detection\n",
    "    threshold = (0.01 * len(df_data))\n",
    "\n",
    "    # 1) the small clusters with less than a threshold (e.g., 1% of total number of data points)\n",
    "    # 2) isolation data points not belong to any cluster\n",
    "\n",
    "    arr = []\n",
    "    outlier = []\n",
    "    for k, v in final_kmeans.items():\n",
    "        arr.append([k, (np.mean(v, axis=0) + 2 * np.std(v, axis=0)), (np.mean(v, axis=0) - 2 * np.std(v, axis=0))])\n",
    "\n",
    "    clust_dict = dict(sorted(final_kmeans.items()))\n",
    "    clustr_count = {}\n",
    "    for k, v in clust_dict.items():\n",
    "        clustr_count[k] = len(v)\n",
    "    # print(clustr_count)\n",
    "    # Length od cluster is 1, consider them as isolated points and append to #outliers.\n",
    "\n",
    "    for key, value in clustr_count.items():\n",
    "        if value == 1:\n",
    "            outlier.append(value)\n",
    "\n",
    "        elif value < threshold:\n",
    "            for i in clust_dict[key]:\n",
    "                outlier.append(i)\n",
    "    \n",
    "    # a data point belongs to a cluster with more than 2 standard deviations #(i.e., 95% confidence)\n",
    "\n",
    "    for i, j in zip(clust_dict.keys(), sorted(arr)):\n",
    "        for y in final_kmeans[i]:\n",
    "            if (np.greater(y, arr[i][1]).all()) and ~(np.less(y, arr[i][2]).any()):\n",
    "                outlier.append(y)\n",
    "    \n",
    "    # Final outliers of a dataset\n",
    "    for outlr in outlier:\n",
    "        print(outlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
